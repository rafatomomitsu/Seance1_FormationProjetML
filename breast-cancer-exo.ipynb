{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Breast cancer Wisconsin","metadata":{}},{"cell_type":"markdown","source":"## Librairies utiles","metadata":{}},{"cell_type":"code","source":"# Directive pour afficher les graphiques dans Jupyter\n%matplotlib inline","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Pandas : librairie de manipulation de données\n# NumPy : librairie de calcul scientifique\n# MatPlotLib : librairie de visualisation et graphiques\n# SeaBorn : librairie de graphiques avancés\nimport pandas as pd\nimport numpy as np\nfrom matplotlib import pyplot as plt\nimport seaborn as sns","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Le dataset Breast Cancer Wisconsin","metadata":{}},{"cell_type":"markdown","source":"Le dataset est accessible sur :  \nhttps://www.kaggle.com/uciml/breast-cancer-wisconsin-data  \nhttp://archive.ics.uci.edu/ml/datasets/breast+cancer+wisconsin+%28diagnostic%29  \n(on peut utiliser pd.read_table pour lire un fichier .dat)","metadata":{}},{"cell_type":"code","source":"df = pd.read_csv('../input/breast-cancer-wisconsin-data/data.csv')","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"On peut afficher les 10 premières lignes du dataset :","metadata":{}},{"cell_type":"code","source":"df.head(10)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"On a les informations suivantes :\n1) ID number 2) Diagnosis (M = malignant, B = benign) 3-32)\n\nTen real-valued features are computed for each cell nucleus:\n\na) radius (mean of distances from center to points on the perimeter)  \nb) texture (standard deviation of gray-scale values)  \nc) perimeter  \nd) area  \ne) smoothness (local variation in radius lengths)  \nf) compactness (perimeter^2 / area - 1.0)  \ng) concavity (severity of concave portions of the contour)  \nh) concave points (number of concave portions of the contour)  \ni) symmetry  \nj) fractal dimension (\"coastline approximation\" - 1)  \n  \nThe mean, standard error and \"worst\" or largest (mean of the three largest values) of these features were computed for each image, resulting in 30 features. For instance, field 3 is Mean Radius, field 13 is Radius SE, field 23 is Worst Radius.  \n  \nAll feature values are recoded with four significant digits.","metadata":{}},{"cell_type":"code","source":"df.columns","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Pour avoir l'ensemble du tableau, on peut utiliser un affichage au format HTML :","metadata":{}},{"cell_type":"code","source":"from IPython.core.display import HTML # permet d'afficher du code html dans jupyter\ndisplay(HTML(df.head(10).to_html()))","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Données","metadata":{}},{"cell_type":"code","source":"df.shape","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.describe()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.columns","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"La colonne **'Unnamed: 32'** est vide : on va la supprimer ","metadata":{}},{"cell_type":"code","source":"df = df.drop(['Unnamed: 32'], axis=1)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.diagnosis.value_counts()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Visualisations","metadata":{}},{"cell_type":"code","source":"malin = df.diagnosis=='M'\nbenin = df.diagnosis=='B'","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"*jointplot* permet de visualiser dans un plan les distributions d'un couple de paramètres :","metadata":{}},{"cell_type":"code","source":"sns.jointplot(\"perimeter_worst\", \"area_worst\", df, kind='kde');","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.figure(figsize=(12,12))\nsns.kdeplot(df.perimeter_worst, df.area_worst,  shade=True)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"On peut tracer ce type de graphique avec des couleurs :","metadata":{}},{"cell_type":"code","source":"plt.figure(figsize=(12,12))\nsns.kdeplot(df[malin].perimeter_worst, df[malin].area_worst, cmap=\"Reds\",  shade=True, alpha=0.3, shade_lowest=False)\nsns.kdeplot(df[benin].perimeter_worst, df[benin].area_worst, cmap=\"Greens\", shade=True, alpha=0.3, shade_lowest=False)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Les **diagrammes en boîte** (ou **boîtes à moustaches** ou **box plot**) résument quelques caractéristiques de position du caractère étudié (médiane, quartiles, minimum, maximum ou déciles). Ce diagramme est utilisé principalement pour comparer un même caractère dans deux populations de tailles différentes. Il s'agit de tracer un rectangle allant du premier quartile au troisième quartile et coupé par la médiane. On ajoute alors des segments aux extrémités menant jusqu'aux valeurs extrêmes.  \nPar exemple pour la répartion des espèces selon la longueur du sépale :","metadata":{}},{"cell_type":"code","source":"sns.boxplot(x=\"diagnosis\", y=\"perimeter_worst\", data=df)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Les **violins plots** sont similaires aux box plots, excepté qu’ils permettent de montrer la courbe de densité de probabilité des différentes valeurs. Typiquement, les violins plots présentent un marqueur pour la médiane des données et l’écart interquartile, comme dans un box plot standard.","metadata":{}},{"cell_type":"code","source":"sns.violinplot(x=\"diagnosis\", y=\"perimeter_worst\", data=df)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"*FacetGrid* permet de superposer des graphiques selon une ou plusieurs caractéristiques. On crée une structure avec *FacetGrid*, et on trace ensuite les graphiques avec *map*","metadata":{}},{"cell_type":"code","source":"fig = sns.FacetGrid(df, hue=\"diagnosis\", aspect=3, palette=\"Set2\") # aspect=3 permet d'allonger le graphique\nfig.map(sns.kdeplot, \"perimeter_worst\", shade=True)\nfig.add_legend()","metadata":{"scrolled":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"On veut tracer un nuage de points selon le rayon et la texture de la tumeur, en différenciant la couleur des points selon le diagnostic :","metadata":{}},{"cell_type":"code","source":"sns.lmplot(x=\"radius_mean\", y=\"texture_mean\", data=df, fit_reg=False, hue='diagnosis')","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"*pairplot* affiche les nuages de points associés à tous les couples de paramètres :","metadata":{}},{"cell_type":"code","source":"#sns.pairplot(df, hue=\"diagnosis\")","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Machine learning","metadata":{}},{"cell_type":"markdown","source":"On sépare le dataset en deux parties :\n- un ensemble d'apprentissage (entre 70% et 90% des données), qui va permettre d'entraîner le modèle\n- un ensemble de test (entre 10% et 30% des données), qui va permettre d'estimer la pertinence de la prédiction","metadata":{}},{"cell_type":"code","source":"data_train = df.sample(frac=0.8, random_state=1)          # 80% des données avec frac=0.8\ndata_test = df.drop(data_train.index)     # le reste des données pour le test","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"On sépare les données d'apprentissage (*X_train*) et la cible (*y_train*, la colonnes des données *classe*)","metadata":{}},{"cell_type":"code","source":"X_train = data_train.drop(['diagnosis'], axis=1)\ny_train = data_train['diagnosis']\nX_test = data_test.drop(['diagnosis'], axis=1)\ny_test = data_test['diagnosis']","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Régression logistique","metadata":{}},{"cell_type":"markdown","source":"On veut prédire une variable aléatoire $Y$ à partir d'un vecteur de variables explicatives $X=(X_1,...,X_n)$\nOn \n","metadata":{}},{"cell_type":"markdown","source":"La fonction logistique $\\frac{e^{x}}{1+e^{x}}$ varie entre $-\\infty$ et $+\\infty$ pour $x$ variant entre $0$ et $1$.  \nElle est souvent utilisée pour \"mapper\" une probabilité et un espace réel","metadata":{}},{"cell_type":"code","source":"plt.figure(figsize=(9,9))\n\nlogistique = lambda x: np.exp(x)/(1+np.exp(x))   \n\nx_range = np.linspace(-10,10,50)       \ny_values = logistique(x_range)\n\nplt.plot(x_range, y_values, color=\"red\")","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"La régression logistique consiste à trouver une fonction linéaire C(X) qui permette d'estimer la probabilité de $Y=1$ connaissant $X$ :\n$$p(Y=1|X) = \\frac{e^{C(X)}}{1+e^{C(X)}}$$","metadata":{}},{"cell_type":"markdown","source":"Autrement dit, cela revient à trouver une séparation linéaire des caractéristiques qui minimise un critère d'erreur.","metadata":{}},{"cell_type":"markdown","source":"Pour plus de détails, cf par exemple :  \nhttp://eric.univ-lyon2.fr/~ricco/cours/cours/pratique_regression_logistique.pdf","metadata":{}},{"cell_type":"markdown","source":"On peut tracer la courbe de régression logistique pour prédire l'espèce Virginica à partir de la longueur du sépale avec la fonction *lmplot* :","metadata":{}},{"cell_type":"markdown","source":"On veut maintenant prédire l'espèce à partir de toutes les caractéristiques, et évaluer la qualité de cette prédiction en utilisant la régression logistique définie dans la librairie *sklearn* :","metadata":{}},{"cell_type":"code","source":"from sklearn.linear_model import LogisticRegression","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"On entraîne le modèle de régression logistique avec *fit* :","metadata":{}},{"cell_type":"code","source":"lr = LogisticRegression()\nlr.fit(X_train,y_train)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"On peut prédire les valeurs sur l'ensemble de test avec le modèle entraîné :","metadata":{}},{"cell_type":"code","source":"y_lr = lr.predict(X_test)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Score et matrice de confusion","metadata":{}},{"cell_type":"code","source":"from sklearn.metrics import accuracy_score, confusion_matrix","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"La mesure de pertinence compte le nombre de fois où l'algorithme a fait une bonne prédiction (en pourcentage) :","metadata":{}},{"cell_type":"code","source":"lr_score = accuracy_score(y_test, y_lr)\nprint(lr_score)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Une mesure plus fine consiste à compter le nombre de **faux positif** (valeur prédite 1 et réelle 0) et de **vrai négatif** (valeur prédite 0 et réelle 1). On utilise une **matrice de confusion** :","metadata":{}},{"cell_type":"code","source":"# Matrice de confusion\ncm = confusion_matrix(y_test, y_lr)\nprint(cm)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<img src=\"https://i.stack.imgur.com/gKyb9.png\">","metadata":{}},{"cell_type":"markdown","source":"On peut aussi utiliser la méthode **crosstab** de **Pandas** (plutôt que la méthode confusion_matrix de sklearn) pour afficher la matrice de confusion :","metadata":{}},{"cell_type":"code","source":"pd.crosstab(y_test, y_lr, rownames=['Reel'], colnames=['Prediction'], margins=True)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Exercice  \nLe résultat est-il satisfaisant ?  \nQuel pourrait être le problème ?","metadata":{}},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Arbres de décision","metadata":{}},{"cell_type":"code","source":"fig = sns.FacetGrid(df, hue=\"diagnosis\", aspect=3) # aspect=3 permet d'allonger le graphique\nfig.map(sns.kdeplot, \"perimeter_worst\", shade=True)\nfig.add_legend()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fig = sns.FacetGrid(df[df.perimeter_worst>110], hue=\"diagnosis\", aspect=3) # aspect=3 permet d'allonger le graphique\nfig.map(sns.kdeplot, \"texture_mean\", shade=True)\nfig.add_legend()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fig = sns.FacetGrid(df[(df.perimeter_worst>110) & (df.texture_mean>17)], hue=\"diagnosis\", aspect=3) # aspect=3 permet d'allonger le graphique\nfig.map(sns.kdeplot, \"concave points_mean\", shade=True)\nfig.add_legend()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Un arbre de décision permet de faire à chaque étape un choix entre deux possibilités, pour arriver à une réponse sur les feuilles (cf. Akinator)  \n<img src=\"https://fr.akinator.com/bundles/elokencesite/images/akitudes_670x1096/defi.png?v95\">","metadata":{}},{"cell_type":"markdown","source":"Pour construire un arbre de décision à partir d'un ensemble d'apprentissage, on va choisir une variable qui sépare l'ensemble en deux parties les plus distinctes en fonction d'un critère. Sur les iris par exemple, on peut utiliser la largeur du pétale pour séparer l'espèce Setosa des autres.","metadata":{}},{"cell_type":"markdown","source":"L'indice *GINI* mesure avec quelle fréquence un élément aléatoire de l'ensemble serait mal classé si son étiquette était sélectionnée aléatoirement depuis la distribution des étiquettes dans le sous-ensemble.","metadata":{}},{"cell_type":"code","source":"from sklearn import tree\ndtc = tree.DecisionTreeClassifier()\ndtc.fit(X_train,y_train)\ny_dtc = dtc.predict(X_test)\nprint(accuracy_score(y_test, y_dtc))","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.figure(figsize=(30,30))\ntree.plot_tree(dtc, feature_names=X_train.columns, class_names=['benin','malin'], fontsize=14, filled=True)  ","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"On peut modifier certains paramètres :  Le paramètre *max_depth* est un seuil sur la profondeur maximale de l’arbre. Le paramètre *min_samples_leaf* donne le nombre minimal d’échantillons dans un noeud feuille.","metadata":{}},{"cell_type":"code","source":"dtc1 = tree.DecisionTreeClassifier(max_depth = 3, min_samples_leaf = 20)\ndtc1.fit(X_train,y_train)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"On obtient un arbre un peu différent :","metadata":{}},{"cell_type":"code","source":"plt.figure(figsize=(30,30))\ntree.plot_tree(dtc1, feature_names=X_train.columns, class_names=['benin','malin'], fontsize=14, filled=True)  ","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y_dtc1 = dtc1.predict(X_test)\nprint(accuracy_score(y_test, y_dtc1))","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Pour plus de détails sur les arbres de décision :  \nhttps://zestedesavoir.com/tutoriels/962/les-arbres-de-decisions/comprendre-le-concept/#1-les-origines  \nhttp://cedric.cnam.fr/vertigo/Cours/ml2/tpArbresDecision.html  \nhttp://perso.mines-paristech.fr/fabien.moutarde/ES_MachineLearning/Slides/coursFM_AD-RF.pdf  ","metadata":{}},{"cell_type":"markdown","source":"## Random forests","metadata":{}},{"cell_type":"markdown","source":"<img src=\"https://infinitescript.com/wordpress/wp-content/uploads/2016/08/Random-Forest-Example.jpg\">","metadata":{}},{"cell_type":"markdown","source":"cf par exemple :  \nhttps://fr.wikipedia.org/wiki/For%C3%AAt_d%27arbres_d%C3%A9cisionnels  \nhttps://www.biostars.org/p/86981/  \nhttps://infinitescript.com/2016/08/random-forest/","metadata":{}},{"cell_type":"code","source":"from sklearn import ensemble\nrf = ensemble.RandomForestClassifier()\nrf.fit(X_train, y_train)\ny_rf = rf.predict(X_test)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"rf_score = accuracy_score(y_test, y_rf)\nprint(rf_score)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Et la matrice de confusion :","metadata":{}},{"cell_type":"code","source":"pd.crosstab(y_test, y_rf, rownames=['Reel'], colnames=['Prediction'], margins=True)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"On a un *faux positif* et deux *faux négatifs*","metadata":{}},{"cell_type":"markdown","source":"### Importance des caractéristiques","metadata":{}},{"cell_type":"markdown","source":"L'attribut *feature_importances_* renvoie un tableau du poids de chaque caractéristique dans la décision :","metadata":{}},{"cell_type":"code","source":"importances = rf.feature_importances_\nindices = np.argsort(importances)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"On peut visualiser ces degrés d'importance avec un graphique à barres par exemple :","metadata":{}},{"cell_type":"code","source":"plt.figure(figsize=(12,8))\nplt.barh(range(len(indices)), importances[indices], color='b', align='center')\nplt.yticks(range(len(indices)), df.columns[indices])\nplt.title('Importance des caracteristiques')","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Exercice : les iris","metadata":{}},{"cell_type":"markdown","source":"Le dataset des iris est prédéfini dans seaborn :","metadata":{}},{"cell_type":"code","source":"df = sns.load_dataset(\"iris\")","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.head()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"On a les informations suivantes :\n- longueur du sépale (en cm)\n- largeur du sépale\n- longueur du pétale\n- largeur du pétale\n- espèce : Virginica, Setosa ou Versicolor","metadata":{}},{"cell_type":"markdown","source":"<img src=\"https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcQM3aH4Q3AplfE1MR3ROAp9Ok35fafmNT59ddXkdEvNdMkT8X6E\">","metadata":{}},{"cell_type":"code","source":"sns.pairplot(df, hue=\"species\")","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Appliquer les méthodes de visualisation et de machine learning vues précédemment sur ce dataset","metadata":{}},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}